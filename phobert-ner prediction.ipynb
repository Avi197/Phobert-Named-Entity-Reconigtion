{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5982c959",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File \"/home/phamson/VnCoreNLP/VnCoreNLP-1.1.1.jar\" was not found, please check again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use VnCoreNLP word segmentation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvncorenlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VnCoreNLP\n\u001b[0;32m----> 3\u001b[0m annotator \u001b[38;5;241m=\u001b[39m \u001b[43mVnCoreNLP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/phamson/VnCoreNLP/VnCoreNLP-1.1.1.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwseg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Xmx500m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.8/site-packages/vncorenlp/vncorenlp.py:45\u001b[0m, in \u001b[0;36mVnCoreNLP.__init__\u001b[0;34m(self, address, port, timeout, annotators, max_heap_size, quiet)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Check if VnCoreNLP file exists\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(address):\n\u001b[0;32m---> 45\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m was not found, please check again.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m address)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Check if VnCoreNLPServer file exists\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(VNCORENLP_SERVER):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File \"/home/phamson/VnCoreNLP/VnCoreNLP-1.1.1.jar\" was not found, please check again."
     ]
    }
   ],
   "source": [
    "# use VnCoreNLP word segmentation\n",
    "from vncorenlp import VnCoreNLP\n",
    "annotator = VnCoreNLP(\"/home/phamson/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5fe079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa8f13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tạo label list tương ứng với bộ dữ liệu\n",
    "# label_list = [\n",
    "#     \"O\",       # Outside of a named entity\n",
    "#     \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
    "#     \"I-MISC\",  # Miscellaneous entity\n",
    "#     \"B-PER\",   # Beginning of a person's name right after another person's name\n",
    "#     \"I-PER\",   # Person's name\n",
    "#     \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
    "#     \"I-ORG\",   # Organisation\n",
    "#     \"B-LOC\",   # Beginning of a location right after another location\n",
    "#     \"I-LOC\"    # Location\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54513a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import sent_tokenize, word_tokenize\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c53c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base', use_fast=False)\n",
    "phobert_ner = AutoModelForTokenClassification.from_pretrained('/opt/github/Phobert-Named-Entity-Reconigtion/phobert-ner-mck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12f70f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo label list tương ứng với bộ dữ liệu\n",
    "# label_list = [\n",
    "#     \"B-CDT\",\n",
    "#     \"B-DA\",\n",
    "#     \"I-CDT\",\n",
    "#     \"I-DA\",\n",
    "#     \"O\"\n",
    "\n",
    "# ]\n",
    "\n",
    "label_list = [\n",
    "    \"B-CNAME\",\n",
    "    \"B-HNAME\",\n",
    "    \"I-CNAME\",\n",
    "    \"I-HNAME\",\n",
    "    \"MCK\",\n",
    "    \"O\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974249a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/phamson/data/real estate news/1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m txt_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/phamson/data/real estate news/1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtxt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m         sentences \u001b[38;5;241m=\u001b[39m annotator\u001b[38;5;241m.\u001b[39mtokenize(line)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/phamson/data/real estate news/1.txt'"
     ]
    }
   ],
   "source": [
    "txt_file = '/home/phamson/data/real estate news/1.txt'\n",
    "\n",
    "with open(txt_file, 'r') as file:\n",
    "    for line in file:\n",
    "        sentences = annotator.tokenize(line)\n",
    "        for sentence in sentences:\n",
    "            sequence = \" \".join(sentence)  # tạo câu mới đã được tokenized \n",
    "            input_ids = torch.tensor([tokenizer.encode(sequence)])  # lấy id của các tokens tương ứng \n",
    "            # không dùng tokenize(decode(encode)), text sẽ bị lỗi khi tokenize do conflict với tokenizer mặc định\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(sequence))  # lấy các token để đánh tags \n",
    "\n",
    "            outputs = phobert_ner(input_ids).logits\n",
    "            predictions = torch.argmax(outputs, dim=2)\n",
    "            print(predictions)\n",
    "\n",
    "            for i in [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())]:\n",
    "                print(i)\n",
    "            print('---------------------------------------------')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9d61e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (261 > 256). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (261) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [1, 261].  Tensor sizes: [1, 258]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# không dùng tokenize(decode(encode)), text sẽ bị lỗi khi tokenize do conflict với tokenizer mặc định\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(tokenizer\u001b[38;5;241m.\u001b[39mencode(tokenize_sentence))  \u001b[38;5;66;03m# lấy các token để đánh tags \u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mphobert_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     94\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [(token, label_list[prediction]) \u001b[38;5;28;01mfor\u001b[39;00m token, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tokens, predictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())]:\n",
      "File \u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/github/transformers/src/transformers/models/roberta/modeling_roberta.py:1404\u001b[0m, in \u001b[0;36mRobertaForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1416\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1418\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/ner_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/github/transformers/src/transformers/models/roberta/modeling_roberta.py:817\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    816\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 817\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (261) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [1, 261].  Tensor sizes: [1, 258]"
     ]
    }
   ],
   "source": [
    "# text = \"Ngày 27/4, UBND TPHCM tổ chức lễ công bố quyết định của Thủ tướng Chính phủ công nhận huyện Cần Giờ đạt chuẩn nông thôn mới năm 2020 và công nhận xã đảo Thạnh An là xã đảo thuộc TPHCM.\"\n",
    "\n",
    "# text = \"Chủ tịch UBND TPHCM Nguyễn Thành Phong nhấn mạnh: đây là sự kiện quan trọng đánh dấu bước phát triển mới của huyện Cần Giờ, mở ra nhiều cơ hội để thúc đẩy phát triển kinh tế trong thời gian tới.\"\n",
    "\n",
    "# text = \"Thạnh An là xã đảo của huyện cần Giờ cách trung tâm huyện khoảng 8km tính theo đường chim bay và cách trung tâm TPHCM khoảng 50 km. Tổng diện tích tự nhiên cùa xã là 13.130 ha, chiếm 18,6% diện tích tự nhiên cùa huyện (ảnh: Trung Kiên).\"\n",
    "\n",
    "# text = 'Đặc biệt, với việc công nhận Thạnh An là xã đảo, người dân Thạnh An sẽ được hưởng nhiều cơ chế chính sách hơn cũng như các điều kiện thuận lợi để thúc đẩy phát triển kinh tế và nhanh chóng bắt kịp các xã khác của huyện Cần Giờ.'\n",
    "#\n",
    "# \"Cuối năm 2020, thành phố vui mừng đón nhận Cù lao Gò Gia được Chính phủ công nhận thuộc TPHCM sau hơn 40 năm chồng lấn địa giới hành chính với tỉnh Đồng Nai. Và nay, Thủ tướng Chính phủ công nhận huyện Cần Giờ đạt chuẩn nông thôn mới năm 2020 và công nhận xã Thạnh An là xã đảo thuộc TPHCM\", Chủ tịch UBND TPHCM bày tỏ.\\\n",
    "# Theo người đứng đầu chính quyền thành phố, con đường phía trước của Cần Giờ đang mở ra với nhiều cơ hội và thách thức đan xen, đòi hỏi Đảng bộ, Chính quyền và Nhân dân huyện Cần Giờ phải chủ động, sáng tạo, quyết liệt và nỗ lực không ngừng vươn lên.'\n",
    "# text = \"Đặc_biệt , với việc công_nhận Thạnh_An là xã đảo , người_dân Thạnh_An sẽ được hưởng nhiều cơ_chế chính_sách hơn cũng như các điều_kiện thuận_lợi để thúc_đẩy phát_triển kinh_tế và nhanh_chóng bắt kịp các xã khác của huyện Cần_Giờ .\"\n",
    "\n",
    "# text = 'Đặc biệt, với việc công nhận Thạnh An là xã đảo, người dân Thạnh An sẽ được hưởng nhiều cơ chế chính sách hơn cũng như các điều kiện thuận lợi để thúc đẩy phát triển kinh tế và nhanh chóng bắt kịp các xã khác của huyện Cần Giờ.'\n",
    "# text = \"Ngày 27/4, UBND TPHCM tổ chức lễ công bố quyết định của Thủ tướng Chính phủ công nhận huyện Cần Giờ đạt chuẩn nông thôn mới năm 2020 và công nhận xã đảo Thạnh An là xã đảo thuộc TPHCM.\"\n",
    "# text = \"Chủ tịch UBND TPHCM Nguyễn Thành Phong nhấn mạnh: đây là sự kiện quan trọng đánh dấu bước phát triển mới của huyện Cần Giờ, mở ra nhiều cơ hội để thúc đẩy phát triển kinh tế trong thời gian tới.\"\n",
    "\n",
    "# text = \"dự án Sun Grand City New An Thới với chủ đầu tư là công ty Sungroup\"\n",
    "# text = \"Sun Grand City New An Thới nằm tiếp giáp đường 975 tuyến giao thông huyết mạch nối liền sân bay Phú Quốc, thị trấn An Thới, đường Nguyễn Văn Cừ, đây cũng là tuyến đường trung tâm kết nối mọi dự án của tập đoàn Sun Group tại đảo Ngọc.\"\n",
    "# text = \"Thông tin mô tả\\\n",
    "# Bán đất chính chủ, sổ đỏ đầy đủ, đường 24m, khu đô thị Hà Phong, Tiền Phong, Mê Linh, Hà Nội.\\\n",
    "# Diện tích 160m2, giá 50 triệu/m², có thương lượng.\\\n",
    "# Khu Đô Thị mới Hà Phong thuộc quần thể ĐTM Bắc Thăng Long, dự án được quy hoạch đạt tiêu chuẩn của KĐT hiện đại với kết cấu hạ tầng tiên tiến và hoàn chỉnh.\\\n",
    "# Từ vị trí KĐT đi vào trung tâm TP Hà Nội rất thuận tiện, chỉ mất 15 phút chạy xe.\"\n",
    "\n",
    "# text = 'Độc quyền suất ngoại giao trực tiếp CĐT: Giá chỉ 25,6tr/m2.\\\n",
    "# TSG Lotus Long Biên: \"Mua nhà ở ngay - nhận ngay sổ hồng\".\\\n",
    "# * Trả trước chỉ 30% được nhận nhà ở ngay.\\\n",
    "# * Hỗ trợ vay vốn tới 70%GTCH trong 25 năm.\\\n",
    "# * Nhận Sổ ngay sau 45 ngày kể từ khi ký HĐ.\\\n",
    "# * Miễn phí sử dụng dịch vụ đến tháng 10/2021.'\n",
    "\n",
    "# text = \"Ban quản lý dự án Vinhomes Ocean Park kính gửi tới quý khách hàng danh sách biệt thự Shophouse bán mới nhất\\\n",
    "# Cập nhật danh sách Biệt thự shophouse bán tuần 2 tháng 4 đầy đủ nhất, giá tốt nhất thị trường.\\\n",
    "# Tổng hợp biệt thự Hải Âu và San Hô. \"\n",
    "\n",
    "# Thông tin chung cư Vinhomes Ocean Park Gia Lâm\\\n",
    "# Vinhomes Ocean Park Gia Lâm được quy hoạch theo mô hình đô thị Singapore, mang đến cho cư dân một môi trường sống lý tưởng cùng nhiều tiện ích hấp dẫn như công viên thể thao ngoài trời, công viên BBQ... Với giải pháp tài chính linh hoạt, dự án hứa hẹn sẽ là giải pháp đột phá về nhà ở cho người dân và là cơ hội đầu tư đầy tiềm năng.\\\n",
    "# - Tên dự án: Vinhomes Ocean Park (Gia Lâm)\\\n",
    "# - Chủ đầu tư: Công TNHH Đầu tư và Phát triển đô thị Gia Lâm\\\n",
    "# - Nhà thầu thi công: Công ty CP Xây dựng Coteccons\\\n",
    "# - Vị trí: Xã Đa Tốn - Kiêu Kỵ - Dương Xá và một phần thị trấn Trâu Qùy, huyện Gia Lâm, TP. Hà Nội\\\n",
    "# - Tổng diện tích: Khoảng 420ha\\\n",
    "# - Mật độ xây dựng: 19,2%\\\n",
    "# - Diện tích công viên, cây xanh: 62ha\\\n",
    "# - Diện tích hồ điều hòa: 24,5ha\\\n",
    "# - Biển hồ nước mặn: 6,1ha\\\n",
    "# - Quy mô: 2 phân khu gồm phân khu cao tầng và phân khu thấp tầng\\\n",
    "# - Phân khu thấp tầng: gồm 2.300 căn biệt thự với các loại hình\\\n",
    "# + Biệt thự nhà vườn: 168m2\\\n",
    "# + Biệt thự liền kề: 90-130m2\\\n",
    "# + Biệt thự đơn lập: 250-450m2\\\n",
    "# + Biệt thự song lập: 180-250m2\\\n",
    "# + Shophouse: 170-300m2\\\n",
    "content = \"Vinhomes Ocean Park Gia Lâm được quy hoạch theo mô hình đô thị Singapore, mang đến cho cư dân một môi trường sống lý tưởng cùng nhiều tiện ích hấp dẫn như công viên thể thao ngoài trời, công viên BBQ... Với giải pháp tài chính linh hoạt, dự án hứa hẹn sẽ là giải pháp đột phá về nhà ở cho người dân và là cơ hội đầu tư đầy tiềm năng.\\\n",
    "- Tên dự án: Vinhomes Ocean Park (Gia Lâm)\\\n",
    "- Chủ đầu tư: Công TNHH Đầu tư và Phát triển đô thị Gia Lâm\\\n",
    "- Nhà thầu thi công: Công ty CP Xây dựng Coteccons\\\n",
    "- Vị trí: Xã Đa Tốn - Kiêu Kỵ - Dương Xá và một phần thị trấn Trâu Qùy, huyện Gia Lâm, TP. Hà Nội\\\n",
    "- Tổng diện tích: Khoảng 420ha\\\n",
    "- Mật độ xây dựng: 19,2%\\\n",
    "- Diện tích công viên, cây xanh: 62ha\\\n",
    "- Diện tích hồ điều hòa: 24,5ha\\\n",
    "- Biển hồ nước mặn: 6,1ha\\\n",
    "- Quy mô: 2 phân khu gồm phân khu cao tầng và phân khu thấp tầng\\\n",
    "- Phân khu thấp tầng: gồm 2.300 căn biệt thự với các loại hình\\\n",
    "+ Biệt thự nhà vườn: 168m2\\\n",
    "+ Biệt thự liền kề: 90-130m2\\\n",
    "+ Biệt thự đơn lập: 250-450m2\\\n",
    "+ Biệt thự song lập: 180-250m2\\\n",
    "+ Shophouse: 170-300m2\\\n",
    "- Phân khu cao tầng: gồm 4 tiểu khu (The Park, The River, The Sea, The Lake) với 66 tòa cao 23-26 tầng, các căn hộ có diện tích 35-85m2, từ 1-3 phòng ngủ\"\n",
    "\n",
    "# text = \"thành phố Hà Nội\"\n",
    "\n",
    "# text = \"u hướng vì sức khoẻ ở phân khúc nhà hạng sang\\nThứ tư, 4/3/2020, 08:00 (GMT+7) \\nSunshine Group đầu tư nghìn tỷ cho tòa Aqua Beauty Resort nhằm đáp ứng nhu cầu căn hộ nghỉ dưỡng giới nhà giàu trong bối cảnh môi trường ngày càng ô nhiễm.\\n\\n\\nCuối năm ngoái, Sunshine Group tung ra thị trường tổ hợp tổ hợp resort Sunshine Diamond River nằm trên đường Đào Trí, quận 7 giáp siêu dự án 6 tỷ USD Peninsula. Aqua Beauty Resort là một trong 8 tòa tháp của dự án 1,2 tỷ USD này.\\n\\nChủ đầu tư không giấu tham vọng biến hạng mục trở thành trung tâm làm đẹp, trị liệu hàng đầu tại Việt Nam. Tòa tháp được thiết kế cảnh quan như một resort tại Bali với không gian cây xanh mặt nước bố trí tận thềm căn hộ. Chủ đầu tư chi cả nghìn tỷ đồng chỉ phát triển hàng trăm tiện ích resort cho toà tháp như: dòng sông nhân tạo dài gần 1.000m hồ chân mây, hơn 40 khu vườn nhiệt đới, hàng chục vườn thiền, hàng chục hồ bơi trên cao, 4.000 vườn nhiệt đới mini trên không, quảng trường ánh sáng, sân trượt băng, khu phố mua sắm trong công viên, chuỗi Bungalow nghỉ dưỡng trên mặt nước.\\n\\n\\n\\n\\nPhối cảnh dòng sông nhân tạo dài gần 1.000m của Aqua Beauty Resort nhìn từ trên cao.\\n\\n\\nKhách hàng của sản phẩm là giới siêu giàu. Những cư dân thành thị không thể mua cây trồng khắp thành phố, nhưng có thể trồng rất nhiều cây trong khuôn viên dự án chính họ làm ra. \\\"Tôi không thể bảo vệ cư dân khỏi ô nhiễm ngoài đường, nhưng về nhà, họ sẽ tăng cường sức khỏe qua các tiện ích. Cư dân không có điều kiện du lịch thường xuyên, tôi sẽ khiến nơi ở của họ giống như resort\\\", vị đại diện chủ đầu tư chia sẻ về lý do Sunshine theo đuổi dòng căn hộ chăm sóc sức khỏe - wellness.\\n\\nNgoài ra, Sunshine Group đầu tư gần 20 tiện ích chuyên phục vụ trị liệu, nghỉ dưỡng, làm đẹp và thanh lọc cơ thể tại tòa tháp. Trong đó, một khu đại spa hiện đại và quy mô rộng hàng nghìn m2 bố trí tại tầng đế của tòa tháp, với 4 phân khu: aqua spa, formal spa, detox center cùng hệ thống vườn trị liệu. Bên cạnh công nghệ làm đẹp, tòa tháp sở hữu trung tâm thanh lọc cơ thể (detox) quy mô lớn, bao gồm: chuỗi phòng detox thuỷ liệu, quang học và khí detox; chuỗi nhà hàng và cửa hàng chuyên bán và cung cấp các liệu trình detox cho cư dân, trung tâm khí liệu chuyên dạy yoga và phương pháp dưỡng sinh.\\n\\nThực tế Aqua Beauty Resort là sản phẩm tiếp nối mô hình \\\"wellness\\\" trong lĩnh địa ốc tại Việt Nam thời gian qua. Theo giới đầu tư, khái niệm wellness gắn với các không gian sống và làm việc được thiết kế với mục tiêu chú trọng vào sức khỏe thể chất và tinh thần.\\n\\nManh nha từ một tổ hợp dự án nghỉ dưỡng tại Đà Nẵng hồi những năm 2016-2017, đến 2018 với mô hình này bùng nổ khi lan nhanh sang đến dự án căn hộ cao cấp tại các đô thị lớn. Diamond Island của Kusto Home, The Vista Verde của Capitaland tại khu Đông, Green Star Sky Garden (Hưng Lộc Phát)... những dự án đóng góp cho sự sôi động của phân khúc này trên thị trường TP HCM. Tại Hà Nội, Ecopark Grand - The Island của Tập đoàn Ecopark lại tận dụng lợi thế thiên nhiên sẵn có để tạo dựng một không gian sống đúng chuẩn wellness.\\n\\nTheo báo cáo Build Well to Live Well, thị trường bất động sản tích hợp các tiện ích hướng đến mục tiêu chăm sóc sức khoẻ (wellness real estate) trên thế giới hiện có giá trị 134 tỷ USD và ước tính sẽ tăng lên 180 tỷ USD vào năm 2022, chiếm khoảng 50% so với ngành công nghiệp xây dựng \\\"xanh\\\" trên toàn cầu.\\n\\nMột báo cáo từ Savills cũng đánh giá sản phẩm \\\"wellness\\\" là xu hướng mới trong thị trường bất động sản nghỉ dưỡng toàn cầu với sự xuất hiện của các wellness resort, spa resort. Và tại Việt Nam, mô hình này còn rất mới.\\n\\n\\n\\n\\nPhối cảnh Detox Center quy mô lớn trong lòng dự án.\\n\\n\\nTiến sĩ Trần Nguyễn Minh Hải - chuyên gia địa ốc Đại học Ngân hàng TP HCM cho rằng thị trường địa ốc đang theo xu hướng phát triển bền vững. Yếu tố khỏe sẽ là tiêu chí ưu tiên khi chọn sản phẩm. Do đó, mô hình căn hộ hướng đến sức khỏe (wellness real estate) đòi hỏi tiềm lực tài chính của mỗi chủ đầu tư. Khi người dân có nhu cầu, thị trường sẽ phải chuyển dịch. \\\"Đây được xem như tiêu chí quan trọng quyết định việc xuống tiền của nhóm khách hàng trung và thượng lưu\\\", vị chuyên gia nói.\\n\\nQuang Minh, một nhà đầu tư tại quận 8, TP HCM, cho rằng xu hướng \\\"wellness\\\" nâng tầm giá trị cho mỗi căn hộ và là sản phẩm có tiềm năng đầu tư trong tương lai bởi bắt trúng nhu cầu của những người giàu có.\\n\\nThanh Hương\"\n",
    "\n",
    "\n",
    "\n",
    "sentences = word_tokenize(content)\n",
    "\n",
    "tokenize_sentences = []\n",
    "for paragraph in paragraphs:\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    for sentence in sentences:\n",
    "#         sentence = self.remove_url(sentence)\n",
    "        if sentence != '':\n",
    "            tokenize_sentence = word_tokenize(sentence, format=\"text\")\n",
    "#             tokenize_sentences.append(tokenize_sentence)\n",
    "            input_ids = torch.tensor([tokenizer.encode(tokenize_sentence)])  # lấy id của các tokens tương ứng \n",
    "            # không dùng tokenize(decode(encode)), text sẽ bị lỗi khi tokenize do conflict với tokenizer mặc định\n",
    "#             tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(tokenize_sentence))  # lấy các token để đánh tags \n",
    "\n",
    "#             outputs = phobert_ner(input_ids).logits\n",
    "#             predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "    \n",
    "#     for i in [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())]:\n",
    "#         print(i)\n",
    "#     print('---------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8962c93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 'O')\n",
      "('FPT', 'MCK')\n",
      "('cho', 'O')\n",
      "('biết', 'O')\n",
      "(',', 'O')\n",
      "('doanh_thu', 'O')\n",
      "('công_ty', 'O')\n",
      "('tăng', 'O')\n",
      "('tại', 'O')\n",
      "('mọi', 'O')\n",
      "('thị_trường', 'O')\n",
      "(',', 'O')\n",
      "('đặc_biệt', 'O')\n",
      "('tại', 'O')\n",
      "('thị_trường', 'O')\n",
      "('Mỹ', 'O')\n",
      "('(', 'O')\n",
      "('+', 'O')\n",
      "('6@@', 'O')\n",
      "('0,7', 'O')\n",
      "('%', 'O')\n",
      "(')', 'O')\n",
      "('và', 'O')\n",
      "('thị_trường', 'O')\n",
      "('AP@@', 'O')\n",
      "('AC', 'O')\n",
      "('(', 'O')\n",
      "('+', 'O')\n",
      "('41', 'O')\n",
      "('%', 'O')\n",
      "(')', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n",
      "---------------------------------------------\n",
      "('<s>', 'O')\n",
      "('Doanh_thu', 'O')\n",
      "('chuyển_đổi', 'O')\n",
      "('số', 'O')\n",
      "('trong', 'O')\n",
      "('3', 'O')\n",
      "('tháng', 'O')\n",
      "('đầu', 'O')\n",
      "('năm', 'O')\n",
      "('đạt', 'O')\n",
      "('1.6@@', 'O')\n",
      "('48', 'O')\n",
      "('tỷ', 'O')\n",
      "('đồng', 'O')\n",
      "(',', 'O')\n",
      "('tăng_trưởng', 'O')\n",
      "('9@@', 'O')\n",
      "('6,2', 'O')\n",
      "('%', 'O')\n",
      "('so', 'O')\n",
      "('với', 'O')\n",
      "('cùng', 'O')\n",
      "('kỳ', 'O')\n",
      "(',', 'O')\n",
      "('tập_trung', 'O')\n",
      "('vào', 'O')\n",
      "('các', 'O')\n",
      "('công_nghệ', 'O')\n",
      "('mới', 'O')\n",
      "('như', 'O')\n",
      "('Điện_toán', 'O')\n",
      "('đám_@@', 'O')\n",
      "('mây', 'O')\n",
      "('(', 'O')\n",
      "('tăng', 'O')\n",
      "('308', 'O')\n",
      "('%', 'O')\n",
      "(')', 'O')\n",
      "(',', 'O')\n",
      "('Trí_tuệ', 'O')\n",
      "('nhân_tạo', 'O')\n",
      "('/', 'O')\n",
      "('Phân_tích', 'O')\n",
      "('dữ_liệu', 'O')\n",
      "('(', 'O')\n",
      "('tăng', 'O')\n",
      "('gần', 'O')\n",
      "('200', 'O')\n",
      "('%', 'O')\n",
      "(')', 'O')\n",
      "('và', 'O')\n",
      "('Lo@@', 'O')\n",
      "('w_@@', 'O')\n",
      "('code', 'O')\n",
      "('(', 'O')\n",
      "('tăng', 'O')\n",
      "('137', 'O')\n",
      "('%', 'O')\n",
      "(')', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenize_sentences:\n",
    "    list_ids = tokenizer(sentence)['input_ids']\n",
    "\n",
    "#         if len(list_ids) >= 256:\n",
    "#             list_ids = list_ids[0:255]\n",
    "    input_ids = torch.tensor([list_ids])\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(list_ids)\n",
    "    outputs = phobert_ner(input_ids).logits\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    for i in [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())]:\n",
    "        print(i)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b60ff725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4981, -1.7473, -2.3166, -3.3962, -3.0136,  6.3143],\n",
       "         [-2.4347, -2.1935, -2.7712, -3.1141, -3.0071,  7.3433],\n",
       "         [-1.4981, -1.7473, -2.3166, -3.3962, -3.0137,  6.3143]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "68e7b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"Công ty Cổ phần Dầu Thực vật Tường An (mã CK: TAC) đã công bố BCTC quý 1/2022. Cụ thể, doanh thu thuần đạt 1.697 tỷ đồng tăng 7,2% so với cùng kỳ. Tuy nhiên giá vốn hàng bán chiếm tới 94% trong doanh thu khiến lãi gộp chỉ còn hơn 98 tỷ đồng, giảm 55% so với quý 1/2021. Trong kỳ chi phí bán hàng giảm mạnh từ 113,5 tỷ đồng xuống còn hơn 21 tỷ đồng, chi phí QLDN cũng thấp hơn cùng kỳ. Do lãi gộp thấp nên kết quả TAC vẫn báo lãi sau thuế giảm 32% so với cùng kỳ, đạt 53 tỷ đồng – tương đương EPS đạt 1.559 đồng.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f285d435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = word_tokenize(content)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2742a8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', 'O')\n",
      "('Công_ty', 'B-CNAME')\n",
      "('Cổ_phần', 'I-CNAME')\n",
      "('Dầu', 'I-CNAME')\n",
      "('Thực_vật', 'I-CNAME')\n",
      "('Tường_@@', 'I-CNAME')\n",
      "('An', 'I-CNAME')\n",
      "('(', 'O')\n",
      "('mã', 'O')\n",
      "('CK', 'O')\n",
      "(':', 'O')\n",
      "('TAC', 'MCK')\n",
      "(')', 'O')\n",
      "('đã', 'O')\n",
      "('công_bố', 'O')\n",
      "('BCTC', 'O')\n",
      "('quý', 'O')\n",
      "('1/@@', 'O')\n",
      "('2022', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n",
      "---------------------------------------------\n",
      "('<s>', 'O')\n",
      "('Cụ_thể', 'O')\n",
      "(',', 'O')\n",
      "('doanh_thu', 'O')\n",
      "('thuần', 'O')\n",
      "('đạt', 'O')\n",
      "('1.6@@', 'O')\n",
      "('97', 'O')\n",
      "('tỷ', 'O')\n",
      "('đồng', 'O')\n",
      "('tăng', 'O')\n",
      "('7,2', 'O')\n",
      "('%', 'O')\n",
      "('so', 'O')\n",
      "('với', 'O')\n",
      "('cùng', 'O')\n",
      "('kỳ', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n",
      "---------------------------------------------\n",
      "('<s>', 'O')\n",
      "('Tuy_nhiên', 'O')\n",
      "('giá', 'O')\n",
      "('vốn', 'O')\n",
      "('hàng', 'O')\n",
      "('bán', 'O')\n",
      "('chiếm', 'O')\n",
      "('tới', 'O')\n",
      "('94', 'O')\n",
      "('%', 'O')\n",
      "('trong', 'O')\n",
      "('doanh_thu', 'O')\n",
      "('khiến', 'O')\n",
      "('lãi', 'O')\n",
      "('gộp', 'O')\n",
      "('chỉ', 'O')\n",
      "('còn', 'O')\n",
      "('hơn', 'O')\n",
      "('98', 'O')\n",
      "('tỷ', 'O')\n",
      "('đồng', 'O')\n",
      "(',', 'O')\n",
      "('giảm', 'O')\n",
      "('55', 'O')\n",
      "('%', 'O')\n",
      "('so', 'O')\n",
      "('với', 'O')\n",
      "('quý', 'O')\n",
      "('1/@@', 'O')\n",
      "('2021', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n",
      "---------------------------------------------\n",
      "('<s>', 'O')\n",
      "('Trong', 'O')\n",
      "('kỳ', 'O')\n",
      "('chi_phí', 'O')\n",
      "('bán', 'O')\n",
      "('hàng', 'O')\n",
      "('giảm', 'O')\n",
      "('mạnh', 'O')\n",
      "('từ', 'O')\n",
      "('1@@', 'O')\n",
      "('13,5', 'O')\n",
      "('tỷ', 'O')\n",
      "('đồng', 'O')\n",
      "('xuống', 'O')\n",
      "('còn', 'O')\n",
      "('hơn', 'O')\n",
      "('21', 'O')\n",
      "('tỷ', 'O')\n",
      "('đồng', 'O')\n",
      "(',', 'O')\n",
      "('chi_phí', 'O')\n",
      "('QL@@', 'O')\n",
      "('DN', 'O')\n",
      "('cũng', 'O')\n",
      "('thấp', 'O')\n",
      "('hơn', 'O')\n",
      "('cùng', 'O')\n",
      "('kỳ', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n",
      "---------------------------------------------\n",
      "('<s>', 'O')\n",
      "('Do', 'O')\n",
      "('lãi', 'O')\n",
      "('gộp', 'O')\n",
      "('thấp', 'O')\n",
      "('nên', 'O')\n",
      "('kết_quả', 'O')\n",
      "('TAC', 'MCK')\n",
      "('vẫn', 'O')\n",
      "('báo', 'O')\n",
      "('lãi', 'O')\n",
      "('sau', 'O')\n",
      "('thuế', 'O')\n",
      "('giảm', 'O')\n",
      "('32', 'O')\n",
      "('%', 'O')\n",
      "('so', 'O')\n",
      "('với', 'O')\n",
      "('cùng', 'O')\n",
      "('kỳ', 'O')\n",
      "(',', 'O')\n",
      "('đạt', 'O')\n",
      "('53', 'O')\n",
      "('tỷ', 'O')\n",
      "('đồng_@@', 'O')\n",
      "('–', 'O')\n",
      "('tương_đương', 'O')\n",
      "('EPS', 'O')\n",
      "('đạt', 'O')\n",
      "('1.5@@', 'O')\n",
      "('59', 'O')\n",
      "('đồng', 'O')\n",
      "('.', 'O')\n",
      "('</s>', 'O')\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# content = self.replace_special_character(content)\n",
    "\n",
    "# split các đoạn văn\n",
    "paragraphs = [p for p in content.split(\"\\n\") if p != '']\n",
    "\n",
    "tokenize_sentences = []\n",
    "for paragraph in paragraphs:\n",
    "    # split sentence with underthesea sent_tokenize\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    for sentence in sentences:\n",
    "        if sentence != '':\n",
    "            # tokenize the sentence\n",
    "            tokenize_sentence = word_tokenize(sentence, format=\"text\")\n",
    "            tokenize_sentences.append(tokenize_sentence)\n",
    "\n",
    "for sentence in tokenize_sentences:\n",
    "    list_ids = tokenizer(sentence)['input_ids']\n",
    "\n",
    "#         if len(list_ids) >= 256:\n",
    "#             list_ids = list_ids[0:255]\n",
    "    input_ids = torch.tensor([list_ids])\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(list_ids)\n",
    "    outputs = phobert_ner(input_ids).logits\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    for i in [(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())]:\n",
    "        print(i)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bee442c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "082d0871",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Độc_quyền', 'O'),\n",
       "  ('suất', 'O'),\n",
       "  ('ngoại_giao', 'O'),\n",
       "  ('trực_tiếp', 'O'),\n",
       "  ('CĐT', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('Giá', 'O'),\n",
       "  ('chỉ', 'O'),\n",
       "  ('25,6', 'O'),\n",
       "  ('tr', 'O'),\n",
       "  ('/', 'O'),\n",
       "  ('m2', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('TSG', 'O'),\n",
       "  ('Lotus_Long_Biên', 'B-PER'),\n",
       "  (':', 'O'),\n",
       "  ('\"', 'O'),\n",
       "  ('Mua', 'O'),\n",
       "  ('nhà', 'O'),\n",
       "  ('ở', 'O'),\n",
       "  ('ngay', 'O'),\n",
       "  ('-', 'O'),\n",
       "  ('nhận', 'O'),\n",
       "  ('ngay', 'O'),\n",
       "  ('sổ', 'O'),\n",
       "  ('hồng', 'O'),\n",
       "  ('\"', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('*', 'O'),\n",
       "  ('Trả', 'O'),\n",
       "  ('trước', 'O'),\n",
       "  ('chỉ', 'O'),\n",
       "  ('30%', 'O'),\n",
       "  ('được', 'O'),\n",
       "  ('nhận', 'O'),\n",
       "  ('nhà', 'O'),\n",
       "  ('ở', 'O'),\n",
       "  ('ngay', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('*', 'O'),\n",
       "  ('Hỗ_trợ', 'O'),\n",
       "  ('vay', 'O'),\n",
       "  ('vốn', 'O'),\n",
       "  ('tới', 'O'),\n",
       "  ('70%', 'O'),\n",
       "  ('GTCH', 'O'),\n",
       "  ('trong', 'O'),\n",
       "  ('25', 'O'),\n",
       "  ('năm', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('*', 'O'),\n",
       "  ('Nhận_Sổ', 'O'),\n",
       "  ('ngay', 'O'),\n",
       "  ('sau', 'O'),\n",
       "  ('45', 'O'),\n",
       "  ('ngày', 'O'),\n",
       "  ('kể', 'O'),\n",
       "  ('từ', 'O'),\n",
       "  ('khi', 'O'),\n",
       "  ('ký', 'O'),\n",
       "  ('HĐ', 'O'),\n",
       "  ('.', 'O')],\n",
       " [('*', 'O'),\n",
       "  ('Miễn_phí', 'O'),\n",
       "  ('sử_dụng', 'O'),\n",
       "  ('dịch_vụ', 'O'),\n",
       "  ('đến', 'O'),\n",
       "  ('tháng', 'O'),\n",
       "  ('10/2021.', 'O')]]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VncoreNLP \n",
    "ner = annotator.ner(text)\n",
    "\n",
    "ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53c5b87",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting underthesea\n",
      "  Downloading underthesea-1.3.4-py3-none-any.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from underthesea) (1.0.2)\n",
      "Requirement already satisfied: PyYAML in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from underthesea) (6.0)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
      "\u001b[K     |████████████████████████████████| 235 kB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from underthesea) (1.1.0)\n",
      "Requirement already satisfied: requests in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from underthesea) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from underthesea) (4.64.0)\n",
      "Collecting underthesea-core==0.0.4_alpha.10\n",
      "  Downloading underthesea_core-0.0.4_alpha.10-cp38-cp38-manylinux2010_x86_64.whl (580 kB)\n",
      "\u001b[K     |████████████████████████████████| 580 kB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Click>=6.0 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from underthesea) (8.1.2)\n",
      "Collecting python-crfsuite>=0.9.6\n",
      "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from nltk->underthesea) (2022.3.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from requests->underthesea) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from requests->underthesea) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from requests->underthesea) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from requests->underthesea) (3.3)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from scikit-learn->underthesea) (1.19.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from scikit-learn->underthesea) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/anaconda3/envs/ner_env/lib/python3.8/site-packages (from scikit-learn->underthesea) (3.1.0)\n",
      "Installing collected packages: unidecode, underthesea-core, python-crfsuite, nltk, underthesea\n",
      "Successfully installed nltk-3.7 python-crfsuite-0.9.8 underthesea-1.3.4 underthesea-core-0.0.4a10 unidecode-1.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2584dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
